# MEDD
Complete the MEDDOPLACE sharing task


Using the pre-trained language model ("bert-base-multilingual-cased") specifically applied to Spanish in Hugging Face, the context semantic information is captured by the pre-trained language model, and then Bi-LSTM is used to extract features dependently, and finally a gating unit operation is designed in the model to eliminate the useless parameters in the model and try to learn useful feature representations



The prediction file that needs to be submitted is in the taikula-bert-bilstm-gate.01 archive in the root directory
